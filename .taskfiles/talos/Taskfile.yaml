---
version: "3"

vars:
  SYSTEM_UPGRADE_KS: '{{.KUBERNETES_DIR}}/apps/system/system-upgrade-controller/ks.yaml'

env:
  KUBERNETES_VERSION:
    sh: yq '.spec.postBuild.substitute.KUBERNETES_VERSION | select(.)' {{.SYSTEM_UPGRADE_KS}}
  TALOS_VERSION:
    sh: yq '.spec.postBuild.substitute.TALOS_VERSION | select(.)' {{.SYSTEM_UPGRADE_KS}}
  TALOS_SCHEMATIC_ID:
    sh: yq '.spec.postBuild.substitute.TALOS_SCHEMATIC_ID | select(.)' {{.SYSTEM_UPGRADE_KS}}

tasks:

  apply-node:
    desc: Apply Talos config to a node [CLUSTER=main] [IP=required]
    cmds:
      - task: down
      - minijinja-cli {{.TALOS_DIR}}/{{.IP}}.yaml.j2 | op inject | talosctl --nodes {{.IP}} apply-config {{if eq .INSECURE "true"}}--insecure{{end}} --mode={{.MODE}} --file /dev/stdin
      - talosctl --nodes {{.IP}} health
      - task: up
    vars:
      MODE: '{{.MODE | default "auto"}}'
      INSECURE:
        sh: talosctl --nodes {{.NODE}} get machineconfig &> /dev/null && echo false || echo true
    requires:
      vars: [CLUSTER, IP]
    preconditions:
      - op user get --me
      - talosctl --nodes {{.IP}} get machineconfig
      - test -f {{.TALOS_DIR}}/{{.IP}}.yaml.j2
      - test -f {{.KUBERNETES_DIR}}/talosconfig
      - which minijinja-cli op talosctl

  upgrade-node:
    desc: Upgrade Talos on a single node [IP=required]
    cmds:
      - task: down
      - talosctl --nodes {{.IP}} upgrade --image={{.TALOS_IMAGE}} --timeout=10m
      - task: up
    vars:
      MACHINE_TYPE:
        sh: talosctl --nodes {{.IP}} get machinetypes --output=jsonpath='{.spec}'
      TALOS_IMAGE:
        sh: |-
          talosctl --nodes {{.IP}} get machineconfig --output=jsonpath='{.spec}' \
            | yq '.machine.install.image | select(. != null)'
    requires:
      vars: [NODE]
    preconditions:
      - talosctl config info --talosconfig {{.KUBERNETES_DIR}}/talosconfig
      - talosctl --nodes {{.IP}} get machineconfig
      - which minijinja-cli talosctl yq

  upgrade-k8s:
    desc: Upgrade Kubernetes across the whole cluster [CLUSTER=main] [VERSION=required]
    cmds:
      - task: down
      - talosctl --nodes {{.TALOS_CONTROLLER}} upgrade-k8s --to $KUBERNETES_VERSION
      - task: up
    vars:
      TALOS_CONTROLLER:
        sh: talosctl config info --output json | jq --raw-output '.endpoints[]' | shuf -n 1
    requires:
      vars: [CLUSTER]
    preconditions:
      - curl -fsSL -o /dev/null --fail https://github.com/siderolabs/kubelet/releases/tag/$KUBERNETES_VERSION
      - talosctl config info --talosconfig {{.KUBERNETES_DIR}}/talosconfig
      - talosctl --nodes {{.TALOS_CONTROLLER}} get machineconfig
      - test -f {{.KUBERNETES_DIR}}/talosconfig
      - which jq talosctl

  reboot-node:
    desc: Reboot Talos on a single node [CLUSTER=main] [IP=required] [MODE=default]
    cmds:
      - task: down
      - talosctl --nodes {{.IP}} reboot --mode={{.MODE}}
      - talosctl --nodes {{.IP}} health
      - task: up
    vars:
      MODE: '{{.MODE | default "default"}}'
    requires:
      vars: [CLUSTER, IP]
    preconditions:
      - talosctl config info --talosconfig {{.KUBERNETES_DIR}}/talosconfig
      - talosctl --nodes {{.IP}} get machineconfig
      - test -f {{.KUBERNETES_DIR}}/talosconfig
      - which talosctl

  shutdown-cluster:
    desc: Shutdown Talos across the whole cluster [CLUSTER=main]
    prompt: Shutdown the Talos cluster '{{.CLUSTER}}' ... continue?
    cmd: talosctl shutdown --nodes {{.IP_ADDRS}} --force
    vars:
      IP_ADDRS:
        sh: talosctl --talosconfig {{.KUBERNETES_DIR}}/talosconfig config info --output json | jq --join-output '[.nodes[]] | join(",")'
    requires:
      vars: [CLUSTER]
    preconditions:
      - talosctl config info --talosconfig {{.KUBERNETES_DIR}}/talosconfig
      - test -f {{.KUBERNETES_DIR}}/talosconfig
      - which jq kubectl talosctl

  reset-node:
    desc: Reset Talos on a single node [CLUSTER=main] [IP=required]
    prompt: Reset Talos node '{{.IP}}' on the '{{.CLUSTER}}' cluster ... continue?
    cmd: talosctl reset --nodes {{.IP}} --graceful=false
    requires:
      vars: [CLUSTER, IP]
    preconditions:
      - talosctl config info --talosconfig {{.KUBERNETES_DIR}}/talosconfig
      - talosctl --nodes {{.IP}} get machineconfig
      - test -f {{.KUBERNETES_DIR}}/talosconfig

  reset-cluster:
    desc: Reset Talos across the whole cluster [CLUSTER=main]
    prompt: Reset the Talos cluster '{{.CLUSTER}}' ... continue?
    cmd: talosctl reset --nodes {{.IP_ADDRS}} --graceful=false
    vars:
      IP_ADDRS:
        sh: talosctl --talosconfig {{.KUBERNETES_DIR}}/talosconfig config info --output json | jq --join-output '[.nodes[]] | join(",")'
    requires:
      vars: [CLUSTER]
    preconditions:
      - talosctl config info --talosconfig {{.KUBERNETES_DIR}}/talosconfig
      - test -f {{.KUBERNETES_DIR}}/talosconfig
      - which jq talosctl

  kubeconfig:
    desc: Generate the kubeconfig for a Talos cluster [CLUSTER=main]
    cmd: talosctl kubeconfig --nodes {{.TALOS_CONTROLLER}} --force --force-context-name {{.CLUSTER}} {{.KUBERNETES_DIR}}
    vars:
      TALOS_CONTROLLER:
        sh: talosctl --talosconfig {{.KUBERNETES_DIR}}/talosconfig config info --output json | jq --raw-output '.endpoints[]' | shuf -n 1
    requires:
      vars: [CLUSTER]
    preconditions:
      - talosctl config info --talosconfig {{.KUBERNETES_DIR}}/talosconfig
      - test -f {{.KUBERNETES_DIR}}/talosconfig
      - which jq talosctl

  defrag:
    desc: Defrag ETCD database on an single node [CLUSTER=main] [IP=required]
    cmd: talosctl etcd defrag --nodes {{.IP}}
    requires:
      vars: [CLUSTER, IP]
    preconditions:
      - talosctl config info --talosconfig {{.KUBERNETES_DIR}}/talosconfig
      - talosctl --nodes {{.IP}} get machineconfig
      - test -f {{.KUBERNETES_DIR}}/talosconfig
      - which talosctl

  apply-cluster:
    desc: Apply Talos config across the whole cluster [CLUSTER=main] [MODE=default]
    cmds:
      - for: { var: IP_ADDRS }
        task: apply-node
        vars:
          IP: '{{.ITEM}}'
          MODE: '{{.MODE}}'
          CLUSTER: '{{.CLUSTER}}'
    vars:
      MODE: '{{.MODE | default "auto"}}'
      IP_ADDRS:
        sh: talosctl --talosconfig {{.KUBERNETES_DIR}}/talosconfig config info --output json | jq --join-output '[.nodes[]] | join(" ")' 
    requires:
      vars: [CLUSTER]
    preconditions:
      - which jq talosctl

  upgrade-cluster:
    desc: Upgrade Talos across the whole cluster [CLUSTER=main]
    cmds:
      - for: { var: IP_ADDRS }
        task: upgrade-node
        vars:
          IP: '{{.ITEM}}'
          CLUSTER: '{{.CLUSTER}}'
    vars:
      IP_ADDRS:
        sh: talosctl --talosconfig {{.KUBERNETES_DIR}}/talosconfig config info --output json | jq --join-output '[.nodes[]] | join(" ")'
    requires:
      vars: [CLUSTER]
    preconditions:
      - which jq talosctl

  reboot-cluster:
    desc: Reboot the whole Talos cluster [CLUSTER=main] [MODE=default]
    cmds:
      - for: { var: IP_ADDRS }
        task: reboot-node
        vars:
          IP: '{{.ITEM}}'
          MODE: '{{.MODE}}'
          CLUSTER: '{{.CLUSTER}}'
    vars:
      IP_ADDRS:
        sh: talosctl --talosconfig {{.KUBERNETES_DIR}}/talosconfig config info --output json | jq --join-output '[.nodes[]] | join(" ")'
    requires:
      vars: [CLUSTER]
    preconditions:
      - which jq talosctl

  defrag-cluster:
    desc: Defrag ETCD database accross the whole cluster [CLUSTER=main]
    cmds:
      - for: { var: IP_ADDRS }
        task: defrag
        vars:
          IP: '{{.ITEM}}'
          CLUSTER: '{{.CLUSTER}}'
    vars:
      IP_ADDRS:
        sh: talosctl --talosconfig {{.KUBERNETES_DIR}}/talosconfig config info --output json | jq --join-output '[.nodes[]] | join(" ")'
    requires:
      vars: [CLUSTER]
    preconditions:
      - which jq talosctl

  down:
    internal: true
    cmds:
      - '{{if eq .CLUSTER "main"}}until kubectl wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done{{end}}'
      - until kubectl wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
    preconditions:
      - which kubectl

  up:
    internal: true
    cmds:
      - '{{if eq .CLUSTER "main"}}until kubectl wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done{{end}}'
      - until kubectl wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
    preconditions:
      - which kubectl
