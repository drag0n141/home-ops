---
version: '3'

vars:
  SYSTEM_UPGRADE_KS: '{{.KUBERNETES_DIR}}/apps/system/system-upgrade-controller/ks.yaml'

env:
  KUBERNETES_VERSION:
    sh: yq '.spec.postBuild.substitute.KUBERNETES_VERSION | select(.)' {{.SYSTEM_UPGRADE_KS}}
  TALOS_VERSION:
    sh: yq '.spec.postBuild.substitute.TALOS_VERSION | select(.)' {{.SYSTEM_UPGRADE_KS}}
  TALOS_SCHEMATIC_ID:
    sh: yq '.spec.postBuild.substitute.TALOS_SCHEMATIC_ID | select(.)' {{.SYSTEM_UPGRADE_KS}}

tasks:

  talos:
    desc: Bootstrap Talos
    prompt: Bootstrap Talos ...?
    cmds:
      - until talosctl --nodes {{.RANDOM_CONTROLLER}} bootstrap; do sleep 5; done
    vars:
      RANDOM_CONTROLLER:
        sh: talosctl config info --talosconfig {{.KUBERNETES_DIR}}/talosconfig --output json | jq --raw-output '.endpoints[]' | shuf -n 1
    preconditions:
      - talosctl config info --talosconfig {{.KUBERNETES_DIR}}/talosconfig
      - talosctl --nodes {{.RANDOM_CONTROLLER}} get machineconfig
      - which jq talosctl

  apps:
    desc: Bootstrap Apps [CLUSTER=main] [ROOK_DISK=WD_BLACK_SN850X]
    cmds:
      - defer: talosctl kubeconfig --nodes {{.RANDOM_CONTROLLER}} --force {{.KUBERNETES_DIR}}
      - until kubectl wait nodes --for=condition=Ready=False --all --timeout=10m; do sleep 5; done
      - for: { var: BLOCK_DEVICES }
        cmd: talosctl --nodes {{.KEY}} wipe disk {{.ITEM}}
      - op inject --in-file {{.SHARED_DIR}}/bootstrap/resources/secrets.yaml.tpl | kubectl apply --server-side --filename -
      - kubectl --namespace flux-system apply --server-side --filename {{.SHARED_DIR}}/meta/components/substitutions/cluster-settings.yaml
      - helmfile --file {{.SHARED_DIR}}/bootstrap/helmfile.yaml apply --skip-diff-on-install --suppress-diff --kube-context {{.CLUSTER}}
    vars:
      BLOCK_DEVICES_RAW:
        sh: |-
          talosctl get disks --talosconfig {{.KUBERNETES_DIR}}/talosconfig --output json | jq --slurp --compact-output '
            map(select(.spec.model == "{{.ROOK_DISK | default "WD_BLACK_SN850X"}}"))
              | group_by(.node)
              | map({ (.[0].node): (map(.metadata.id) | join(" ")) })
              | add'
      BLOCK_DEVICES:
        ref: fromJson .BLOCK_DEVICES_RAW
      RANDOM_CONTROLLER:
        sh: talosctl config info --talosconfig {{.KUBERNETES_DIR}}/talosconfig --output json | jq --raw-output '.endpoints[]' | shuf -n 1
    env:
      VAULT: kubernetes
      CLUSTER: '{{.CLUSTER}}'
      KUBERNETES_DIR: '{{.KUBERNETES_DIR}}'
      SHARED_DIR: '{{.SHARED_DIR}}'
    requires:
      vars: [CLUSTER, ROOK_DISK]
    preconditions:
      - op user get --me
      - talosctl config info --talosconfig {{.KUBERNETES_DIR}}/talosconfig
      - test -f {{.KUBERNETES_DIR}}/talosconfig
      - test -f {{.SHARED_DIR}}/bootstrap/helmfile.yaml
      - test -f {{.SHARED_DIR}}/bootstrap/resources/secrets.yaml.tpl
      - test -f {{.SHARED_DIR}}/meta/components/substitutions/cluster-settings.yaml
      - which helmfile jq kubectl op talosctl

  matchbox:
    desc: Sync required Matchbox config to PXEBoot machine [CLUSTER=main]
    cmds:
      - ssh -l root 192.168.254.2 "sudo mkdir -p /mnt/nvme/appdata/matchbox/{groups,profiles,assets}"
      - ssh -l root 192.168.254.2 "sudo mkdir -p /mnt/nvme/appdata/matchbox/assets/{{.CLUSTER}}"
      - for: ["kernel-amd64", "initramfs-amd64.xz"]
        cmd: curl -skL https://factory.talos.dev/image/$TALOS_SCHEMATIC_ID/$TALOS_VERSION/{{.ITEM}} | curl --key /root/.ssh/id_ed25519 -skT - -u "root:" sftp://192.168.254.2//mnt/nvme/appdata/matchbox/assets/{{.CLUSTER}}/{{.ITEM}}
      - for: { var: ASSETS }
        cmd: minijinja-cli {{.ITEM}} | op inject | curl --key /root/.ssh/id_ed25519 -skT - -u "root:" sftp://192.168.254.2//mnt/nvme/appdata/matchbox/assets/{{.CLUSTER}}/{{base .ITEM | replace ".yaml.j2" ".yaml"}}
      - for: { var: GROUPS }
        cmd: minijinja-cli {{.ITEM}} | curl --key /root/.ssh/id_ed25519 -skT - -u "root:" sftp://192.168.254.2//mnt/nvme/appdata/matchbox/groups/{{base .ITEM | replace ".json.j2" ".json"}}
      - for: { var: PROFILES }
        cmd: minijinja-cli {{.ITEM}} | curl --key /root/.ssh/id_ed25519 -skT - -u "root:" sftp://192.168.254.2//mnt/nvme/appdata/matchbox/profiles/{{base .ITEM | replace ".json.j2" ".json"}}
      - ssh -l root 192.168.254.2 "docker restart matchbox"
    vars:
      ASSETS:
        sh: ls {{.TALOS_DIR}}/*.yaml.j2
      GROUPS:
        sh: ls {{.KUBERNETES_DIR}}/bootstrap/matchbox/groups/*.json.j2
      PROFILES:
        sh: ls {{.KUBERNETES_DIR}}/bootstrap/matchbox/profiles/*.json.j2
    requires:
      vars: [CLUSTER]
    preconditions:
      - op user get --me
      - ping -c1 192.168.254.2
      - which ls minijinja-cli op
