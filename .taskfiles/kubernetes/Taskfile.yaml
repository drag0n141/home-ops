---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

vars:
  KUBERNETES_RESOURCES_DIR: "{{.ROOT_DIR}}/.taskfiles/kubernetes/resources"

tasks:

  apply-ks:
    desc: Apply a Flux Kustomization resource for a cluster [CLUSTER=main] [PATH=required] [NS=flux-system]
    cmd: >
      flux build --namespace {{.NS}} ks {{base .PATH}}
      --kustomization-file {{.CLUSTER_DIR}}/apps/{{.PATH}}/ks.yaml
      --path {{.CLUSTER_DIR}}/apps/{{.PATH}}
      {{- if contains "not found" .KS }}--dry-run \{{ end }}
      | yq 'with(select(.apiVersion == "kustomize.toolkit.fluxcd.io/v1" and .kind == "Kustomization"); .metadata.namespace = "{{.NS}}")' -
      | kubectl apply --server-side --field-manager=kustomize-controller --filename -
    requires:
      vars: [CLUSTER, PATH]
    vars:
      NS: '{{.NS | default "flux-system"}}'
      KS:
        sh: flux --namespace {{.NS}} get kustomizations {{base .PATH}} 2>&1
    preconditions:
      - test -f {{.CLUSTER_DIR}}/apps/{{.PATH}}/ks.yaml

  reconcile:
    desc: Force update Flux [CLUSTER=main]
    cmd: flux reconcile -n flux-system kustomization cluster-apps cluster-meta cluster-shared --with-source
    requires:
      vars: [CLUSTER]
    preconditions:
      - which flux

  hr-restart:
    desc: Restart all failed Helm Releases [CLUSTER=main]
    cmds:
      - kubectl get hr --all-namespaces | grep False | awk '{print $2, $1}' | xargs -L1 bash -c 'flux suspend hr $0 -n $1'
      - kubectl get hr --all-namespaces | grep False | awk '{print $2, $1}' | xargs -L1 bash -c 'flux resume hr $0 -n $1'
    requires:
      vars: [CLUSTER]
    preconditions:
      - which kubectl

  browse-pvc:
    desc: Mount a PVC to an temp container [CLUSTER=main] [NS=default] [CLAIM=required]
    interactive: true
    cmd: kubectl browse-pvc --namespace {{.NS}} {{.CLAIM}}
    vars:
      NS: '{{.NS | default "default"}}'
    requires:
      vars: [CLUSTER, CLAIM]
    preconditions:
      - kubectl --namespace {{.NS}} get persistentvolumeclaims {{.CLAIM}}

  drain:
    desc: Drain a node [CLUSTER=main] [NODE=required]
    cmd: kubectl drain {{.NODE}} --ignore-daemonsets --delete-emptydir-data --force
    requires:
      vars: [CLUSTER, NODE]
    preconditions:
      - which kubectl

  delete-failed-pods:
    desc: Delete pods with a Failed/Pending/Succeeded phase [CLUSTER=main]
    cmds:
      - for:
          matrix:
            PHASE: [Failed, Pending, Succeeded]
        cmd: kubectl delete pods --field-selector status.phase={{.ITEM.PHASE}} -A --ignore-not-found=true
    requires:
      vars: [CLUSTER]
    preconditions:
      - which kubectl

  ks-*:
    desc: Suspend or resume Kustomizations [CLUSTER=main]
    cmds:
      - kubectl get ns -o jsonpath='{.items[*].metadata.name}' | xargs -n1 flux {{.STATE}} ks --all --namespace
    vars:
      STATE: '{{index .MATCH 0}}'
    requires:
      vars: [CLUSTER]
    preconditions:
      - '[[ "{{.STATE}}" == "suspend" || "{{.STATE}}" == "resume" ]]'
      - which flux kubectl

  privileged:
    desc: Run a privileged pod [CLUSTER=main] [NODE=required]
    cmd: |
      kubectl run privileged-{{.NODE}} -i --rm --image=null \
        --overrides="$(yq {{.KUBERNETES_RESOURCES_DIR}}/privileged-pod.tmpl.yaml -o=json | envsubst)"
    requires:
      vars: [CLUSTER, NODE]
    preconditions:
      - which kubectl
      - test -f {{.KUBERNETES_RESOURCES_DIR}}/privileged-pod.tmpl.yaml

  scale-*:
    desc: Scale down Apps that are using a NFS Mount [CLUSTER=main]
    cmds: 
      - kubectl get deployments --all-namespaces -l nfsMount=true -o custom-columns="NAMESPACE:.metadata.namespace,NAME:.metadata.name" --no-headers | awk '{print "kubectl scale --replicas 0 deployment/"$2" -n "$1}' | sh
    vars:
      SCALE: '{{index .MATCH 0}}'
    requires:
      vars: [CLUSTER]
    preconditions:
      - '[[ "{{.SCALE}}" == "0" || "{{.SCALE}}" == "1" ]]'
      - which kubectl
