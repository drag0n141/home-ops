---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

vars:
  MATCHBOX_DIR: /var/lib/matchbox
  MATCHBOX_GROUPS_DIR: "{{.MATCHBOX_DIR}}/groups"
  MATCHBOX_PROFILES_DIR: "{{.MATCHBOX_DIR}}/profiles"
  # renovate: datasource=docker depName=ghcr.io/siderolabs/installer
  TALOS_VERSION: v1.8.3
  # renovate: datasource=docker depName=ghcr.io/siderolabs/kubelet
  KUBERNETES_VERSION: v1.31.3

tasks:

  apply-node:
    desc: Apply Talos config to a node [CLUSTER=main] [IP=required]
    dotenv: ['{{.CLUSTER_DIR}}/cluster.env']
    cmds:
      - task: down
      - sops exec-file --input-type yaml --output-type yaml {{.CLUSTER_DIR}}/talos/{{.IP}}.sops.yaml.j2 "minijinja-cli {}" | talosctl --nodes {{.IP}} apply-config --mode={{.MODE}} --file /dev/stdin
      - talosctl --nodes {{.IP}} health --wait-timeout=10m --server=false
      - task: up
    vars:
      MODE: '{{.MODE | default "auto"}}'
    requires:
      vars: [CLUSTER, IP]
    preconditions:
      - talosctl --nodes {{.IP}} get machineconfig
      - test -f {{.CLUSTER_DIR}}/cluster.env
      - test -f {{.CLUSTER_DIR}}/talos/{{.IP}}.sops.yaml.j2
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - which minijinja-cli sops talosctl

  upgrade-node:
    desc: Upgrade Talos on a single node [CLUSTER=main] [IP=required]
    dotenv: ['{{.CLUSTER_DIR}}/cluster.env']
    cmds:
      - task: down
      - talosctl --nodes {{.IP}} upgrade --image="factory.talos.dev/installer{{if eq .TALOS_SECUREBOOT "true"}}-secureboot{{end}}/{{.TALOS_SCHEMATIC_ID}}:$TALOS_VERSION" --timeout=10m
      - talosctl --nodes {{.IP}} health --wait-timeout=10m --server=false
      - task: up
    vars:
      TALOS_SCHEMATIC_ID:
        sh: kubectl get nodes --output json | jq --raw-output '.items[] | select(.status.addresses[] | select(.type == "InternalIP" and .address == "{{.IP}}")) | .metadata.annotations."extensions.talos.dev/schematic"'
      TALOS_SECUREBOOT:
        sh: talosctl --nodes {{.IP}} get securitystate --output=jsonpath='{.spec.secureBoot}'
    requires:
      vars: [CLUSTER, IP]
    preconditions:
      - curl -fsSL -o /dev/null --fail https://github.com/siderolabs/talos/releases/tag/$TALOS_VERSION
      - talosctl --nodes {{.IP}} get machineconfig
      - talosctl config info
      - test -f {{.CLUSTER_DIR}}/cluster.env
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - which kubectl talosctl

  upgrade-k8s:
    desc: Upgrade Kubernetes across the whole cluster [CLUSTER=main] [VERSION=required]
    dotenv: ['{{.CLUSTER_DIR}}/cluster.env']
    cmds:
      - task: down
      - talosctl --nodes {{.TALOS_CONTROLLER}} upgrade-k8s --to $KUBERNETES_VERSION
      - task: up
    vars:
      TALOS_CONTROLLER:
        sh: talosctl config info --output json | jq --raw-output '.endpoints[]' | shuf -n 1
    requires:
      vars: [CLUSTER]
    preconditions:
      - curl -fsSL -o /dev/null --fail https://github.com/siderolabs/kubelet/releases/tag/$KUBERNETES_VERSION
      - talosctl --nodes {{.TALOS_CONTROLLER}} get machineconfig
      - talosctl config info
      - test -f {{.CLUSTER_DIR}}/cluster.env
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - which talosctl

  reboot-node:
    desc: Reboot Talos on a single node [CLUSTER=main] [IP=required]
    cmds:
      - task: down
      - talosctl --nodes {{.IP}} reboot
      - talosctl --nodes {{.IP}} health --wait-timeout=10m --server=false
      - task: up
    requires:
      vars: [CLUSTER, IP]
    preconditions:
      - talosctl --nodes {{.IP}} get machineconfig
      - talosctl config info
      - test -f {{.CLUSTER_DIR}}/cluster.env
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - which talosctl

  shutdown-cluster:
    desc: Shutdown Talos across the whole cluster [CLUSTER=main]
    prompt: Shutdown the Talos cluster '{{.CLUSTER}}' ... continue?
    cmd: talosctl shutdown --nodes {{.IPS}} --force
    vars:
      IPS:
        sh: talosctl config info --output json | jq --join-output '[.nodes[]] | join(",")'
    requires:
      vars: [CLUSTER]
    preconditions:
      - talosctl --nodes {{.NODES}} get machineconfig
      - talosctl config info
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - which jq talosctl

  reset-node:
    desc: Reset Talos on a single node [CLUSTER=main] [IP=required]
    prompt: Reset Talos node '{{.IP}}' on the '{{.CLUSTER}}' cluster ... continue?
    cmd: talosctl reset --nodes {{.IP}} --graceful=false
    requires:
      vars: [CLUSTER, IP]
    preconditions:
      - talosctl --nodes {{.IP}} get machineconfig
      - talosctl config info
      - test -f {{.CLUSTER_DIR}}/talosconfig

  reset-cluster:
    desc: Reset Talos across the whole cluster [CLUSTER=main]
    prompt: Reset the Talos cluster '{{.CLUSTER}}' ... continue?
    cmd: talosctl reset --nodes {{.IPS}} --graceful=false
    vars:
      IPS:
        sh: talosctl config info --output json | jq --join-output '[.nodes[]] | join(",")'
    requires:
      vars: [CLUSTER]
    preconditions:
      - talosctl --nodes {{.NODES}} get machineconfig
      - talosctl config info
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - which jq talosctl

  kubeconfig:
    desc: Generate the kubeconfig for a Talos cluster [CLUSTER=main]
    cmd: talosctl kubeconfig --nodes {{.TALOS_CONTROLLER}} --force --force-context-name {{.CLUSTER}} {{.CLUSTER_DIR}}
    vars:
      TALOS_CONTROLLER:
        sh: talosctl config info --output json | jq --raw-output '.endpoints[]' | shuf -n 1
    requires:
      vars: [CLUSTER]
    preconditions:
      - talosctl config info
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - which talosctl

  down:
    internal: true
    cmds:
      - until kubectl wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done
      - until kubectl wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
    preconditions:
      - which kubectl

  up:
    internal: true
    cmds:
      - until kubectl wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done
      - until kubectl wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
    preconditions:
      - which kubectl

  bootstrap-matchbox:
    desc: Bootstrap required Matchbox configuration for PXE Boot [CLUSTER=main] [MATCHBOX_CLUSTER=utility] [MATCHBOX_NAMESPACE=networking]
    cmds:
      - kubectl exec --context {{.MATCHBOX_CLUSTER}} -n {{.MATCHBOX_NAMESPACE}} -c app {{.MATCHBOX_POD}} -- mkdir -p {{.MATCHBOX_GROUPS_DIR}}
      - kubectl exec --context {{.MATCHBOX_CLUSTER}} -n {{.MATCHBOX_NAMESPACE}} -c app {{.MATCHBOX_POD}} -- mkdir -p {{.MATCHBOX_PROFILES_DIR}}
      - kubectl exec --context {{.MATCHBOX_CLUSTER}} -n {{.MATCHBOX_NAMESPACE}} -c app {{.MATCHBOX_POD}} -- mkdir -p {{.MATCHBOX_ASSETS_DIR}}
      - for: ["kernel-amd64", "initramfs-amd64.xz"]
        cmd: |
          kubectl exec --context {{.MATCHBOX_CLUSTER}} -n {{.MATCHBOX_NAMESPACE}} -c app {{.MATCHBOX_POD}} -- wget -q -O {{.MATCHBOX_ASSETS_DIR}}/{{.ITEM}} https://factory.talos.dev/image/{{.TALOS_SCHEMATIC_ID}}/{{.TALOS_VERSION}}/{{.ITEM}}
      - find {{.CLUSTER_DIR}}/bootstrap/talos/matchbox/assets -type f | xargs -I{} sh -c "sops -d {{.CLUSTER_DIR}}/bootstrap/talos/matchbox/assets/\$(basename {}) | envsubst > {{.CLUSTER_DIR}}/bootstrap/talos/matchbox/assets/\$(basename {} | sed 's/\.secret\.sops//')"
#      - find {{.CLUSTER_DIR}}/bootstrap/talos/matchbox/assets -type f -not -name "*.secret.sops.yaml" | xargs -I{} sh -c "kubectl cp --context {{.MATCHBOX_CLUSTER}} {{.CLUSTER_DIR}}/bootstrap/talos/matchbox/assets/\$(basename {}) -c app {{.MATCHBOX_NAMESPACE}}/{{.MATCHBOX_POD}}:{{.MATCHBOX_ASSETS_DIR}}/" | find {{.CLUSTER_DIR}}/bootstrap/talos/matchbox/assets -type f -not -name "*.secret.sops.yaml" -delete 
      - find {{.CLUSTER_DIR}}/bootstrap/talos/matchbox/groups -type f | xargs -I{} kubectl cp --context {{.MATCHBOX_CLUSTER}} {} -c app {{.MATCHBOX_NAMESPACE}}/{{.MATCHBOX_POD}}:{{.MATCHBOX_GROUPS_DIR}}/
      - find {{.CLUSTER_DIR}}/bootstrap/talos/matchbox/profiles -type f | xargs -I{} kubectl cp --context {{.MATCHBOX_CLUSTER}} {} -c app {{.MATCHBOX_NAMESPACE}}/{{.MATCHBOX_POD}}:{{.MATCHBOX_PROFILES_DIR}}/
      - kubectl delete --context {{.MATCHBOX_CLUSTER}} pod -n {{.MATCHBOX_NAMESPACE}} {{.MATCHBOX_POD}}
    vars:
      MATCHBOX_CLUSTER: '{{.MATCHBOX_CLUSTER | default "utility"}}'
      MATCHBOX_NAMESPACE: '{{.MATCHBOX_NAMESPACE | default "networking"}}'
      MATCHBOX_ASSETS_DIR: "{{.MATCHBOX_DIR}}/assets/{{.CLUSTER}}"
      MATCHBOX_POD:
        sh: kubectl get --context {{.MATCHBOX_CLUSTER}} pod -n {{.MATCHBOX_NAMESPACE}} -l app.kubernetes.io/name=matchbox -o name | awk -F'/' '{print $2}'
    env:
      TALOS_VERSION: "{{.TALOS_VERSION}}"
      KUBERNETES_VERSION: "{{.KUBERNETES_VERSION}}"
    requires:
      vars: [CLUSTER]
    preconditions:
      - which kubectl
